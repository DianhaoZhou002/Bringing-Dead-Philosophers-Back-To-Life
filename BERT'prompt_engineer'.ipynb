{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Author: Dianhao Zhou\n",
        "\n",
        "Date of Creation: Dec. 18th, 2023\n",
        "\n",
        "Last Update: Dec. 18th, 2023"
      ],
      "metadata": {
        "id": "I9duuQz87OMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataPreprocessing(needed here, run before calling BERT)\n",
        "\n"
      ],
      "metadata": {
        "id": "j_XCZPusz8Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "gtjOLPJQz-mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import PyPDF2\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "XakNKDsI0XOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data Preprocessing\n",
        "\n",
        "#unzip\n",
        "def unzip_file(zip_path, extract_folder):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_folder)\n",
        "\n",
        "#remove editorial formats\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in brackets\n",
        "    text = re.sub(r'·.*?·', '', text)    # Remove text in small dots\n",
        "    text = re.sub(r'•', '', text)        # Remove bullets\n",
        "    text = re.sub(r'\\.\\s*\\.\\s*\\.\\s*\\.', '', text)  # Remove ellipses\n",
        "    sentence_tokens = sent_tokenize(text)\n",
        "    text = ' '.join(sent for sent in sentence_tokens if not sent.strip().endswith('?'))\n",
        "    return text\n",
        "\n",
        "#pdf to text\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "        for page_num in range(num_pages):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                cleaned_text = clean_text(page_text)\n",
        "                text += cleaned_text\n",
        "    return text\n",
        "\n",
        "#remove repeated clauses and tokenize into sentences(also can do maxlength here)\n",
        "def preprocess_text(text, mode):\n",
        "    repeated_clauses = ['ESSAYS ON SUICIDE AND THE IMMORTALITY OF THE SOUL',\n",
        "                        'ESSAY II. ON THE IMMORTALITY OF THE SOUL.',\n",
        "                        '\"Enquiry Concerning Human Understanding\"',\n",
        "                        'David Hume',\n",
        "                        'Online Library of Liberty: Essays Moral, Political, Literary (LF ed.)',\n",
        "                        'PLL v6.0 (generated September, 2011)',\n",
        "                        'http://oll.libertyfund.org/title/704',\n",
        "                        'Dialogues concerning Natural Religion',\n",
        "                        'Pamphilus to Hermippus']\n",
        "    for clause in repeated_clauses:\n",
        "        text = re.sub(clause, '', text)\n",
        "\n",
        "    if mode == 'sen':\n",
        "\n",
        "        sentence_tokens = sent_tokenize(text)\n",
        "\n",
        "        return sentence_tokens, text\n",
        "    if mode == 'max':\n",
        "        word_tokens = word_tokenize(text)\n",
        "        segmented_texts = []\n",
        "        current_segment = []\n",
        "\n",
        "        for token in word_tokens:\n",
        "            if len(current_segment) + len(token) <= 30:#let's say 30\n",
        "                current_segment.append(token)\n",
        "            else:\n",
        "                segmented_texts.append(\" \".join(current_segment))\n",
        "                current_segment = [token]\n",
        "\n",
        "        if current_segment:\n",
        "            segmented_texts.append(\" \".join(current_segment))\n",
        "\n",
        "        return segmented_texts, text\n",
        "#read into a pdf dict for further embedding and a text dict to find relevant text\n",
        "def read_pdfs_into_dict(folder_path,mode):\n",
        "    pdf_dict = {}\n",
        "    text_dict = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            text = extract_text_from_pdf(file_path)\n",
        "            preprocessed_text, _ = preprocess_text(text,mode)\n",
        "            pdf_dict[filename] = preprocessed_text\n",
        "            text_dict[filename] = _\n",
        "    return pdf_dict, text_dict\n",
        "\n",
        "def preprocess(zip_path, folder,mode):\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    unzip_file(zip_path, folder)\n",
        "\n",
        "    folder_path = folder + '/' + folder\n",
        "    pdf_text_dict, text_dict = read_pdfs_into_dict(folder_path,mode)\n",
        "    return pdf_text_dict, text_dict\n"
      ],
      "metadata": {
        "id": "Wbb3_NiB0Zd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #find relevant text wit topic word\n",
        "def find_relevant_document(topic_word, documents):\n",
        "    topic_counts = {doc_name: doc_text.count(topic_word) for doc_name, doc_text in documents.items()}\n",
        "    relevant_doc = max(topic_counts, key=topic_counts.get)\n",
        "    return relevant_doc, topic_counts[relevant_doc]\n",
        "\n",
        "def get_relevant_document(topic_word, text_dict, pdf_text_dict):\n",
        "    relevant_document,count = find_relevant_document(topic_word, text_dict)\n",
        "\n",
        "    sentences = []\n",
        "    for segment in pdf_text_dict[relevant_document]:\n",
        "        sentences.append(segment)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "gWLoBQNo0bp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define variables\n",
        "zip_path = \"All.zip\"\n",
        "folder = \"All\"\n",
        "mode = 'sen'\n",
        "#preprocess\n",
        "pdf_text_dict, text_dict = preprocess(zip_path, folder, mode)"
      ],
      "metadata": {
        "id": "3I83mbp60iwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define variables\n",
        "topic_word = \"passion\"\n",
        "user_input = \"what is passion?\"\n",
        "n = 10\n",
        "#relevant sentences\n",
        "sentences = get_relevant_document(topic_word, text_dict, pdf_text_dict)"
      ],
      "metadata": {
        "id": "tFVxyazy0q6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT'prompt_engineer'"
      ],
      "metadata": {
        "id": "QVhWpW0wz9Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finding sentences\n",
        "\n",
        "def bert_top_n(sentences, user_input, n):\n",
        "\n",
        "    #load the tokenizer and model of choise\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    #figure out the divice we are running with\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"Using CPU\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    hume_sentences = sentences\n",
        "    def embed_text(text):# this embedding works for both input and hume texts\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)#tokenize it\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}#get them to devise\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)#and get teh embedding\n",
        "        return outputs.last_hidden_state.mean(dim=1).cpu()\n",
        "\n",
        "    #embed both the input and background texts\n",
        "    user_embedding = embed_text(user_input)\n",
        "    sentence_embeddings = [embed_text(sentence) for sentence in hume_sentences]\n",
        "\n",
        "    #find similarity\n",
        "    similarities = [cosine_similarity(user_embedding.detach().numpy(), sentence_embedding.detach().numpy())[0][0] for sentence_embedding in sentence_embeddings]\n",
        "\n",
        "    #find those top n similar sentences, could be passages if we are runnig with max length preprocessing\n",
        "    top_n_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:n]\n",
        "    top_n_sentences = [hume_sentences[i] for i in top_n_indices]\n",
        "\n",
        "    #outputting a prompt.txt file at the same time for prompt generation\n",
        "    with open('prompts.txt', \"w\") as f:\n",
        "        for sentence in top_n_sentences:\n",
        "            print(sentence)\n",
        "            f.write(sentence + \"\\n\")\n",
        "        f.write(\"||\\n\")  # Delimiter for read_prompt function\n",
        "\n",
        "    return top_n_sentences"
      ],
      "metadata": {
        "id": "kGr1n_O72Knm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# expecting input from DataPreprocessing.ipynb, sentences\n",
        "result = bert_top_n(sentences,user_input,n)"
      ],
      "metadata": {
        "id": "_pbJJygq0A5-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}