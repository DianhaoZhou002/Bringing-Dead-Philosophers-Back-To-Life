{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Author: Dianhao Zhou\n",
        "\n",
        "Date of Creation: Dec. 18th, 2023\n",
        "\n",
        "Last Update: Dec. 18th, 2023"
      ],
      "metadata": {
        "id": "9-gl4PyL6yuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are doing data preprocessing, taking in a .zip file that have all texts in pdfs, and outputting desirable preprocessed workable data."
      ],
      "metadata": {
        "id": "ZST4Yf563CqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "nNqGMqHXvg0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "import zipfile\n",
        "import os\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import PyPDF2\n",
        "import os\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1B1daru_vgrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KfRuSSptvenF"
      },
      "outputs": [],
      "source": [
        "#data Preprocessing\n",
        "\n",
        "#unzip\n",
        "def unzip_file(zip_path, extract_folder):\n",
        "    with zipfile.ZipFile(zip_path + '.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_folder)\n",
        "\n",
        "#remove editorial formats\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in brackets\n",
        "    text = re.sub(r'·.*?·', '', text)    # Remove text in small dots\n",
        "    text = re.sub(r'•', '', text)        # Remove bullets\n",
        "    text = re.sub(r'\\.\\s*\\.\\s*\\.\\s*\\.', '', text)  # Remove ellipses\n",
        "    sentence_tokens = sent_tokenize(text)\n",
        "    text = ' '.join(sent for sent in sentence_tokens if not sent.strip().endswith('?'))\n",
        "    return text\n",
        "\n",
        "#pdf to text\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "        for page_num in range(num_pages):#for each pages\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:#we add cleaned texts\n",
        "                cleaned_text = clean_text(page_text)\n",
        "                text += cleaned_text\n",
        "    return text\n",
        "\n",
        "#remove repeated clauses and tokenize into sentences(also can do maxlength here)\n",
        "def preprocess_text(text, mode):\n",
        "    #need to remove repeated clauses\n",
        "    repeated_clauses = ['ESSAYS ON SUICIDE AND THE IMMORTALITY OF THE SOUL',\n",
        "                        'ESSAY II. ON THE IMMORTALITY OF THE SOUL.',\n",
        "                        '\"Enquiry Concerning Human Understanding\"',\n",
        "                        'David Hume',\n",
        "                        'Online Library of Liberty: Essays Moral, Political, Literary (LF ed.)',\n",
        "                        'PLL v6.0 (generated September, 2011)',\n",
        "                        'http://oll.libertyfund.org/title/704',\n",
        "                        'Dialogues concerning Natural Religion',\n",
        "                        'Pamphilus to Hermippus']\n",
        "    #removal happens here\n",
        "    for clause in repeated_clauses:\n",
        "        text = re.sub(clause, '', text)\n",
        "\n",
        "    #we have two modes, here is tokenizing into sentences\n",
        "    if mode == 'sen':\n",
        "\n",
        "        sentence_tokens = sent_tokenize(text)\n",
        "\n",
        "        return sentence_tokens, text\n",
        "\n",
        "    #here is another, tokenizing accordong to maximum length. We set a default of 30\n",
        "    if mode == 'max':\n",
        "        word_tokens = word_tokenize(text)\n",
        "        segmented_texts = []\n",
        "        current_segment = []\n",
        "\n",
        "        for token in word_tokens:\n",
        "            if len(current_segment) + len(token) <= 30:#let's say 30\n",
        "                current_segment.append(token)\n",
        "            else:\n",
        "                segmented_texts.append(\" \".join(current_segment))\n",
        "                current_segment = [token]\n",
        "\n",
        "        if current_segment:\n",
        "            segmented_texts.append(\" \".join(current_segment))\n",
        "\n",
        "        return segmented_texts, text\n",
        "#read into a pdf dict for further embedding and a text dict to find relevant text\n",
        "def read_pdfs_into_dict(folder_path,mode):\n",
        "    #we prepare pdf_dict which contains tokenized text for further training, text_dict for BERT'prompr engineer'\n",
        "    pdf_dict = {}\n",
        "    text_dict = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            #first extract\n",
        "            text = extract_text_from_pdf(file_path)\n",
        "            #then preprocess, with cleaning process included in it\n",
        "            preprocessed_text, _ = preprocess_text(text,mode)\n",
        "            pdf_dict[filename] = preprocessed_text\n",
        "            text_dict[filename] = _\n",
        "    return pdf_dict, text_dict\n",
        "\n",
        "def preprocess(zip_path, folder,mode):#everything in one\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    unzip_file(zip_path + '.zip', folder)\n",
        "\n",
        "    folder_path = zip_path + '/' + folder\n",
        "    pdf_text_dict, text_dict = read_pdfs_into_dict(folder_path,mode)\n",
        "    return pdf_text_dict, text_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocess for GPT2 fine tuning\n",
        "def for_gpt2(zip_path, folder, mode='sen'):#to parse in sentences by default\n",
        "    pdf_text_dict, _ = preprocess(zip_path, folder, mode)\n",
        "\n",
        "    sentences = []\n",
        "    #now writing into a .txt file for GPT fine tuning\n",
        "    text_data = open('Sentences.txt', 'w')\n",
        "\n",
        "    for filename, segmented_text in pdf_text_dict.items():\n",
        "        print(f\"File: {filename}\")\n",
        "        for i, segment in enumerate(segmented_text):\n",
        "            text_data.write(segment)\n",
        "            sentences.append(segment)\n",
        "\n",
        "    text_data.close()\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "uRak8mYqxXRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip and preprocess the All.zip file example, remember to load the zip before running\n",
        "zip_path = \"All\"\n",
        "folder = \"All\"\n",
        "sentences = for_gpt2(zip_path, folder)"
      ],
      "metadata": {
        "id": "-MW0_bxxyClh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# special preprocess for BERT 'prompt engineer'\n",
        "\n",
        " #find relevant text wit topic word\n",
        "def find_relevant_document(topic_word, documents):\n",
        "    topic_counts = {doc_name: doc_text.count(topic_word) for doc_name, doc_text in documents.items()}#relevance is measured by wordcount\n",
        "    relevant_doc = max(topic_counts, key=topic_counts.get)#find the most relevant one using this metric\n",
        "    return relevant_doc\n",
        "\n",
        "def get_relevant_document(topic_word, text_dict, pdf_text_dict):\n",
        "\n",
        "    #call the find function\n",
        "    relevant_document= find_relevant_document(topic_word, text_dict)\n",
        "\n",
        "    #get the sentences for prompt engineering\n",
        "    sentences = []\n",
        "    for segment in pdf_text_dict[relevant_document]:\n",
        "        sentences.append(segment)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "KBQE9H9gv4E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip and preprocess the All.zip file example, remember to load the zip before running\n",
        "\n",
        "#define variables\n",
        "zip_path = \"All.zip\"\n",
        "folder = \"All\"\n",
        "mode = 'max'\n",
        "#preprocess\n",
        "pdf_text_dict, text_dict = preprocess(zip_path, folder, mode)\n",
        "\n",
        "#define variables\n",
        "topic_word = \"passion\"\n",
        "user_input = \"what is passion?\"\n",
        "n = 5\n",
        "#relevant sentences\n",
        "sentences = get_relevant_document(topic_word, text_dict, pdf_text_dict)"
      ],
      "metadata": {
        "id": "EtVgzFXByjaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#both at once\n",
        "\n",
        "#define variables\n",
        "zip_path = \"All.zip\"\n",
        "folder = \"All\"\n",
        "mode = 'max'\n",
        "#preprocess\n",
        "pdf_text_dict, text_dict = preprocess(zip_path, folder, mode)\n",
        "\n",
        "text_data = open('Sentences.txt', 'w')\n",
        "\n",
        "for filename, segmented_text in pdf_text_dict.items():\n",
        "    print(f\"File: {filename}\")\n",
        "    for i, segment in enumerate(segmented_text):\n",
        "        text_data.write(segment)\n",
        "        sentences.append(segment)\n",
        "\n",
        "text_data.close()\n",
        "\n",
        "#define variables\n",
        "topic_word = \"passion\"\n",
        "user_input = \"what is passion?\"\n",
        "n = 5\n",
        "#relevant sentences\n",
        "sentences = get_relevant_document(topic_word, text_dict, pdf_text_dict)"
      ],
      "metadata": {
        "id": "vnIEVo2IzEq6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}